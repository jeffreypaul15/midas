{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "part_2.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jxvic8LfgIUY"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical # convert to one-hot-encoding\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from keras.models import Model\n",
        "import keras\n",
        "from sklearn.metrics import classification_report\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing import image\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ft2Quk0jgP5K"
      },
      "source": [
        "mnist_model = keras.models.load_model('/content/drive/MyDrive/midas/model_check/training0_9/new_new_50.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EnZEIPAvVDH"
      },
      "source": [
        "From the link provided, the standard MNIST split is considered 16.67% for test data (10000 images as test and 60000 images as train)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEcZqoU5gXWL"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mkKWc9HvaXJ"
      },
      "source": [
        "Ideally we would want to change the input shape of the given model to 28x28 as we are dealing with MNIST data, but instead of having to go through the tedious task of having to resize the model's input layer, I shall resize the mnist data to be of 100x100 which is what the model was trained initially trained on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBTdTXODgYQx"
      },
      "source": [
        "def extract_features(image_data=None):\n",
        "  data = []\n",
        "  for i in image_data:\n",
        "    x = cv2.resize(i, dsize=(100, 100), interpolation=cv2.INTER_CUBIC)\n",
        "    x = np.expand_dims(x, axis=2)\n",
        "    x = np.array(x, dtype='float64')\n",
        "    data.append(x)\n",
        "  return np.asarray(data)\n",
        "    \n",
        "  \n",
        "resized_x_train = extract_features(image_data=x_train)\n",
        "resized_x_test = extract_features(image_data=x_test)\n",
        "resized_y_train = tf.keras.utils.to_categorical(y_train, num_classes=10, dtype='float32')\n",
        "resized_y_test = tf.keras.utils.to_categorical(y_test, num_classes=10, dtype='float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sb9B4_taoEmP"
      },
      "source": [
        "Evaluating MNIST data with previously trained data.\n",
        "Despite the accuracy being lowm it does do a decent job of predicting. \n",
        "This does bring the accuracy up to 55% but still not ideal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MryO5VzjoD_D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d225ff28-9f09-46ec-a928-8d758fb9e90a"
      },
      "source": [
        "y_pred = np.argmax(mnist_model.predict(resized_x_test), axis=1)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.09      0.16       980\n",
            "           1       0.96      0.12      0.21      1135\n",
            "           2       0.40      0.84      0.54      1032\n",
            "           3       0.56      0.83      0.67      1010\n",
            "           4       0.85      0.58      0.69       982\n",
            "           5       0.67      0.56      0.61       892\n",
            "           6       0.64      0.90      0.75       958\n",
            "           7       0.62      0.65      0.63      1028\n",
            "           8       0.36      0.67      0.47       974\n",
            "           9       0.79      0.33      0.47      1009\n",
            "\n",
            "    accuracy                           0.55     10000\n",
            "   macro avg       0.66      0.56      0.52     10000\n",
            "weighted avg       0.67      0.55      0.52     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jec-v7EUgiTK"
      },
      "source": [
        "optimizer = keras.optimizers.RMSprop(learning_rate=0.0001)\n",
        "mnist_model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=\"/content/drive/MyDrive/midas/model_check/mnist_trained_on_previous_new.hdf5\",\n",
        "                                                 save_best_only=True,\n",
        "                                                 verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhbHGnPko3YM"
      },
      "source": [
        "Training 1 epoch on the previously trained model we get the convergence time to be around 5 epochs from where the loss seems to plateau ar 0.03. The data is clearly being overfit at this point in time, but testing is required to be done on the 10000 images that were mentioned.\n",
        "\n",
        "The accuracy obtained from the first epoch itself is around 90% which would lead to a much smaller convergence time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUGpIeRagl6f",
        "outputId": "fd5984f2-753b-4626-cd19-082238170bae"
      },
      "source": [
        "mnist_model.fit(resized_x_train, resized_y_train, verbose = 1, batch_size=32, callbacks=cp_callback, validation_data=(resized_x_test, resized_y_test), epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 35s 18ms/step - loss: 0.3347 - accuracy: 0.9003 - val_loss: 0.0499 - val_accuracy: 0.9864\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.04992, saving model to /content/drive/MyDrive/midas/model_check/mnist_trained_on_previous_new.hdf5\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 33s 17ms/step - loss: 0.0876 - accuracy: 0.9789 - val_loss: 0.0342 - val_accuracy: 0.9903\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.04992 to 0.03424, saving model to /content/drive/MyDrive/midas/model_check/mnist_trained_on_previous_new.hdf5\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 33s 17ms/step - loss: 0.0577 - accuracy: 0.9851 - val_loss: 0.0339 - val_accuracy: 0.9902\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.03424 to 0.03386, saving model to /content/drive/MyDrive/midas/model_check/mnist_trained_on_previous_new.hdf5\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 33s 18ms/step - loss: 0.0457 - accuracy: 0.9889 - val_loss: 0.0321 - val_accuracy: 0.9924\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.03386 to 0.03211, saving model to /content/drive/MyDrive/midas/model_check/mnist_trained_on_previous_new.hdf5\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 33s 17ms/step - loss: 0.0366 - accuracy: 0.9911 - val_loss: 0.0304 - val_accuracy: 0.9922\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.03211 to 0.03042, saving model to /content/drive/MyDrive/midas/model_check/mnist_trained_on_previous_new.hdf5\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 32s 17ms/step - loss: 0.0360 - accuracy: 0.9917 - val_loss: 0.0508 - val_accuracy: 0.9895\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.03042\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 33s 17ms/step - loss: 0.0364 - accuracy: 0.9919 - val_loss: 0.0310 - val_accuracy: 0.9940\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.03042\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 33s 17ms/step - loss: 0.0333 - accuracy: 0.9929 - val_loss: 0.0374 - val_accuracy: 0.9915\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.03042\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 33s 17ms/step - loss: 0.0293 - accuracy: 0.9934 - val_loss: 0.0290 - val_accuracy: 0.9928\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.03042 to 0.02899, saving model to /content/drive/MyDrive/midas/model_check/mnist_trained_on_previous_new.hdf5\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 33s 17ms/step - loss: 0.0277 - accuracy: 0.9938 - val_loss: 0.0325 - val_accuracy: 0.9935\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.02899\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd8a8dac490>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHKtaCdfpfBn"
      },
      "source": [
        "Evaluating the previously trained model (0-9) after training on 1 epoch with the mnist data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvSL880Ppeg9",
        "outputId": "425fdba1-fc91-4fe6-f503-c3818ca3a597"
      },
      "source": [
        "y_pred = np.argmax(mnist_model.predict(resized_x_test), axis=1)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99       980\n",
            "           1       0.99      1.00      1.00      1135\n",
            "           2       1.00      0.98      0.99      1032\n",
            "           3       1.00      0.99      0.99      1010\n",
            "           4       0.99      1.00      0.99       982\n",
            "           5       0.99      0.99      0.99       892\n",
            "           6       1.00      0.98      0.99       958\n",
            "           7       0.99      0.99      0.99      1028\n",
            "           8       0.98      0.99      0.99       974\n",
            "           9       0.99      0.99      0.99      1009\n",
            "\n",
            "    accuracy                           0.99     10000\n",
            "   macro avg       0.99      0.99      0.99     10000\n",
            "weighted avg       0.99      0.99      0.99     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okc1VruBjjly"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n",
        "                 activation ='relu', input_shape = (100, 100, 1)))\n",
        "model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same'))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same'))\n",
        "model.add(LeakyReLU(alpha=0.3))\n",
        "\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same'))\n",
        "model.add(LeakyReLU(alpha=0.3))\n",
        "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same'))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "\n",
        "model.add(Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same'))\n",
        "model.add(LeakyReLU(alpha=0.3))\n",
        "\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "\n",
        "\n",
        "model.add(Conv2D(filters = 256, kernel_size = (3,3),padding = 'Same'))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Conv2D(filters = 512, kernel_size = (3,3),padding = 'Same'))\n",
        "model.add(LeakyReLU(alpha=0.3))\n",
        "\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same'))\n",
        "model.add(LeakyReLU(alpha=0.1))\n",
        "\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation = \"relu\"))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(128, activation = \"relu\"))\n",
        "model.add(Dense(64, activation = \"relu\"))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(10, activation = \"softmax\"))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csfJZBozpEBd"
      },
      "source": [
        "Evaluating the performace agter random initialization. We see the accuracy is much lower in comparison."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_X4il9kEo_Rv",
        "outputId": "1dd47107-6fe1-42f8-9a6f-589b0c345493"
      },
      "source": [
        "y_pred = np.argmax(model.predict(resized_x_test), axis=1)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       980\n",
            "           1       0.00      0.00      0.00      1135\n",
            "           2       0.12      0.01      0.01      1032\n",
            "           3       0.00      0.00      0.00      1010\n",
            "           4       0.10      1.00      0.18       982\n",
            "           5       0.00      0.00      0.00       892\n",
            "           6       0.00      0.00      0.00       958\n",
            "           7       0.00      0.00      0.00      1028\n",
            "           8       0.00      0.00      0.00       974\n",
            "           9       0.00      0.00      0.00      1009\n",
            "\n",
            "    accuracy                           0.10     10000\n",
            "   macro avg       0.02      0.10      0.02     10000\n",
            "weighted avg       0.02      0.10      0.02     10000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7x3-8Kt7aUn"
      },
      "source": [
        "Training from a random model to measure the convergence.\n",
        "We notice that here, the accuracy starts from a much lower & and converges much later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crWAWGvVkD2i",
        "outputId": "320ea27f-b275-49c4-fdbf-baea9d7ba9c0"
      },
      "source": [
        "optimizer = keras.optimizers.RMSprop(learning_rate=0.0001)\n",
        "model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(resized_x_train, resized_y_train, validation_data=(resized_x_test, resized_y_test), verbose = 1, batch_size=32, callbacks=None, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 35s 18ms/step - loss: 1.0560 - accuracy: 0.6306 - val_loss: 0.0527 - val_accuracy: 0.9851\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 33s 17ms/step - loss: 0.1106 - accuracy: 0.9723 - val_loss: 0.0396 - val_accuracy: 0.9893\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 32s 17ms/step - loss: 0.0675 - accuracy: 0.9836 - val_loss: 0.0313 - val_accuracy: 0.9904\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 32s 17ms/step - loss: 0.0484 - accuracy: 0.9882 - val_loss: 0.0309 - val_accuracy: 0.9926\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 32s 17ms/step - loss: 0.0449 - accuracy: 0.9897 - val_loss: 0.0340 - val_accuracy: 0.9922\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 32s 17ms/step - loss: 0.0430 - accuracy: 0.9906 - val_loss: 0.0335 - val_accuracy: 0.9925\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 32s 17ms/step - loss: 0.0396 - accuracy: 0.9912 - val_loss: 0.0343 - val_accuracy: 0.9924\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 33s 17ms/step - loss: 0.0353 - accuracy: 0.9918 - val_loss: 0.0408 - val_accuracy: 0.9888\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 32s 17ms/step - loss: 0.0351 - accuracy: 0.9917 - val_loss: 0.0328 - val_accuracy: 0.9916\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 32s 17ms/step - loss: 0.0316 - accuracy: 0.9927 - val_loss: 0.0614 - val_accuracy: 0.9868\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1e8a35ced0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqeIq1nUj3pr"
      },
      "source": [
        "y_pred = np.argmax(model.predict(resized_x_test), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1cKdcDcnNV8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e759a97-473f-4198-bd30-b6ed80eed2db"
      },
      "source": [
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99       980\n",
            "           1       1.00      0.99      0.99      1135\n",
            "           2       0.95      1.00      0.97      1032\n",
            "           3       0.97      1.00      0.98      1010\n",
            "           4       0.99      0.99      0.99       982\n",
            "           5       1.00      0.96      0.98       892\n",
            "           6       1.00      0.97      0.99       958\n",
            "           7       0.99      0.99      0.99      1028\n",
            "           8       0.99      0.98      0.99       974\n",
            "           9       0.99      0.99      0.99      1009\n",
            "\n",
            "    accuracy                           0.99     10000\n",
            "   macro avg       0.99      0.99      0.99     10000\n",
            "weighted avg       0.99      0.99      0.99     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFSzydxCenf9"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7BAuv717rPe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}